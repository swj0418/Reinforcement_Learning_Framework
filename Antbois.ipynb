{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from environments.env_antworld import Antworld\n",
    "from learning_algorithms.hysteretic_q import HystereticAgent\n",
    "import numpy as np\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Antworld()\n",
    "epochs = 1000\n",
    "simulations = 1\n",
    "epoch_length = 1000\n",
    "num_agents = 1\n",
    "rewards_1 = []\n",
    "dim_x=5\n",
    "dim_y=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pheromone_sums = np.zeros((dim_x,dim_y))\n",
    "ant_sums = np.zeros((dim_x,dim_y))\n",
    "\n",
    "for simulation in range(simulations):\n",
    "\n",
    "    env = Antworld(agents_n=num_agents,dim_x=dim_x,dim_y=dim_y,food=(2,2))\n",
    "    agents = [HystereticAgent(env,this_id) for this_id in range(num_agents)]\n",
    "\n",
    "    # get initial observations/rewards\n",
    "    observations, rewards, _, _ = env.step([],[x for x in range(num_agents)])\n",
    "    #print(observations)\n",
    "    \n",
    "    \n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        env.reset()\n",
    "\n",
    "        sum_rewards = 0\n",
    "        for time in range(epoch_length):\n",
    "            actions = []\n",
    "            for agent_num in range(num_agents):\n",
    "                actions += [agents[agent_num].get_action_from_observation(observations[agent_num]),]\n",
    "\n",
    "            prev_observations = observations\n",
    "            observations, rewards, _, _, = env.step(actions,[x for x in range(num_agents)])\n",
    "            \n",
    "            pheromones, ants = env.pheromone_output()\n",
    "            pheromone_sums += pheromones\n",
    "            ant_sums += ants\n",
    "            \n",
    "            if epoch == epochs-1:\n",
    "                env.display()\n",
    "\n",
    "            sum_rewards += sum(rewards)\n",
    "            for agent_num in range(num_agents):\n",
    "                agents[agent_num].q_learn(prev_observations[agent_num],actions[agent_num],observations[agent_num],rewards[agent_num])\n",
    "            \n",
    "        ##env.display()\n",
    "            \n",
    "        #print(sum_rewards)\n",
    "        rewards_1 += [sum_rewards,]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(list(range(len(rewards_1))),rewards_1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pheromone_sums = pheromone_sums/np.max(pheromone_sums)\n",
    "ant_sums = ant_sums/np.max(ant_sums)\n",
    "im = np.stack([pheromone_sums,np.zeros((dim_x,dim_y)),ant_sums,],axis=-1)\n",
    "print(im.min())\n",
    "# plt.imshow(Image.fromarray(im)).convert('rgb')\n",
    "plt.imshow(im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Anaconda3]",
   "language": "python",
   "name": "conda-env-Anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
